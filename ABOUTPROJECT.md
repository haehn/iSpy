# iSpy
Real Time Collision Avoidance Deep Learning Image Processing Alert System

## Table of Contents


## Introduction
This project presents a real-time collision avoidance alert system using deep learning image processing to trigger a microcontroller. The motivation for this project is to integrate the system with first-person-view drone flight controllers to assist drone pilots in avoiding potential collisions. To predict potential collisions in real time, the system inputs real-time velocity vector field calculations into a convolutional neural network, developed and trained on the NVIDIA™ Jetson Nano, a small artificial intelligence computer. A convolutional neural network is a class of deep learning used for image recognition and classification by applying convolution filter layers to learn features in a dataset for future predictions. Vector field calculations are performed by way of optical flow which is the process of detecting and calculating the movement of pixels between consecutive frames. A threshold was set to trigger an alert to the drone flight controller through a universal asynchronous receiver/transmitter, a device for data transmission. The training model achieved 85.6\% accuracy with 33.6\% loss. When the system was tested, it was able to predict and alert with approximately 72\%.
There is still continuous progress on this project to improve the training model for higher accuracy and lower loss, as well as research on the different parameters of the system.

## Set Up
The hardware components chosen for this project design are the NVIDIA Jetson Nano as the SOM and the Raspberry Pi 4 module v2 camera for the real-time camera stream input. These two main components were used to build the physical system. The Jetson Nano needs the camera for input, while the image processing and UART communication are programmed on the Jetson Nano, completing the system.
The Jetson Nano is an SOM that can run high speed modern AI algorithms, making it a small AI computer. It contains connectors and ports for ethernet, microSD card, HDMI output, DisplayPort, DC barrel jack 5V power input, USB, and MIPI CSI camera  \cite{r1}. The advantage of this SOM is that it can run multiple neural networks in parallel and process several high-resolution sensors simultaneously, which makes it ideal for computer vision and high performance computing. In order to use the Jetson Nano for deep learning image processing, the environment was first assembled with NVIDIA’s JetPack and essential packages and libraries for computer vision \cite{r6}. Python allows for many open-source libraries to be incorporated, such as OpenCV (which has many resources for real-time applications). TensorFlow and Keras were installed. Jupyter Notebook was also installed and used to simulate the algorithms and neural networks before implementing the processing on the Jetson Nano’s GPU. 
The Raspberry Pi module v2 camera is a high resolution camera (3280 x 2464 pixels) that can capture approximately 90 frames per second and is compact in size and compatible with the Jetson Nano. It connects through a ribbon cable to a CSI port, which makes it possible to connect to the Jetson Nano’s MIPI CSI camera connector port \cite{r2}. The Raspberry Pi camera was connected to the Jetson Nano and real-time stream was ensured, using OpenCV source code \cite{r7}, which was later used for iterative testing of the image processing codes.


## Design
The design of this project is based on the goals and the design requirements for the system and specifications of the problem. To meet the goals and requirements of the project the system had to be able to take in a real-time camera stream (placed on a drone or any moving device), process the images to trigger when potential crash objects would be detected, and inform a flight controller of the objects’ location in frame, speed and distance. In order to design and create such a system, the following software and hardware specifications were required: An image processing technique implemented in real-time with high accuracy through the use of neural networks; a small high speed digital camera with high resolution compatible with the chosen SOM; and an SOM that is equipped with a camera input port, ethernet port, portable power supply attachment port, and has high speed processing and large (or expansive) memory. An SOM that is compatible to communicate with the BrainFPV drone’s microcontroller and the software it uses must be open source.

This project’s image processing is based on deep learning neural networks to predict potential collisions with images from real-time camera inputs. Due to the various environments that drones are used in, there are no particular objects to train a neural network with, which makes the use of a CNN model alone is insufficient for the purpose of this project. As a result, an optical flow algorithm was implemented on the frames, making the velocity vector field images the dataset for the neural network’s training model. The expected result of training the machine with dataset of frames processed with optical flow is that the system can learn to detect potential crashes with the information provided from vector field images. This section discusses the design and development of the image processing pipeline.

The image processing software implemented on the Jetson Nano was developed to calculate the velocity vector field between frames using an OpenCV algorithm for the Lucas-Kanade optical flow method \cite{r8}. For the purpose of this project, the algorithm was incorporated in several stages of the design process. Applying the optical flow algorithm to operate in real-time with a camera as the input source pertains to this project because the overall system calls for a real-time camera input. Calculations completed by the algorithm acts as post-processing of the camera capture, extracting information that can not be described from raw images and the sliding window technique was used in conjunction with the optical. The sliding windows technique collects a given number of the most recent frames, and applies optical flow on those frames to calculate the velocity of the pixel movements between the frames. The use of a sliding window technique allows for the combination of multiple optical flow calculations, making each output image a tracking of vector fields throughout several frames, rather than continuously stacking the previous result from the newly calculated two frames. \
 	
As shown in Figure 4, the vector field results for two frames are not continuously being stacked for all frames captured from start to finish by the system. Rather it is doing the processing two frames at a time and stacking the vector field results with results of the next consecutive pair of frames and so on until optical flow is calculated on 5 frames total and that result is a single processed frame. The optical flow images for the training model require preloaded videos as the input. The videos were collected from online drone footage and saved as "jpeg" images to utilize as the dataset. This use of the algorithm allows for the CNN to analyze information that is not attained from the raw camera captures to predict when a crash event will soon occur. \


## Files

